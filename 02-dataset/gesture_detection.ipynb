{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37ed4721-2c0e-4a78-888e-8c586cbd4b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# import a lot of things from keras:\n",
    "# sequential model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# layers\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomContrast, RandomBrightness\n",
    "\n",
    "# loss function\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "# callback functions\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# convert data to categorial vector representation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# nice progress bar for loading data\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# helper function for train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import confusion matrix helper function\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import pre-trained model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# include only those gestures\n",
    "CONDITIONS = ['like', 'dislike', 'rock', 'peace', 'stop']\n",
    "\n",
    "# image size\n",
    "IMG_SIZE = 64\n",
    "SIZE = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# number of color channels we want to use\n",
    "# set to 1 to convert to grayscale\n",
    "# set to 3 to use color images\n",
    "COLOR_CHANNELS = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47b3f618-cd32-48f5-b43d-05d670ec3cba",
   "metadata": {},
   "source": [
    "## helper function to load and parse annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3705021c-b053-4b70-87b1-a0049ba7e6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations = dict()\n",
    "annotations_val = dict()\n",
    "\n",
    "for condition in CONDITIONS:\n",
    "    with open(f'../../gesture_dataset_sample/_annotations/{condition}.json') as f:\n",
    "        annotations[condition] = json.load(f)\n",
    "    with open(f'annot-liu.json') as f:\n",
    "        annotations_val[condition] = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dad5afd-64ee-4dc2-9b34-4429210c3790",
   "metadata": {},
   "source": [
    "## helper function to pre-process images (color channel conversion and resizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e6815af-85fb-483e-ae69-c3a3fee78ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    if COLOR_CHANNELS == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_resized = cv2.resize(img, SIZE)\n",
    "    return img_resized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "121f42a0-ece9-47f3-aefb-521366921c18",
   "metadata": {},
   "source": [
    "## load images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06de9c48-aca0-468b-8048-bc7dca63d3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadfd4e458a34111abef38d27fd54a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dae36b7a6574277afbe47b98b50db5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc88932efc041b983543e41a1d7334e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ace81f487b34813862d6bda01e0f4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8299c90be23e420883a4460e62f10259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b79adc013f41efa4a8bf41267c68e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e45adbaab2e4d1c88e38da88f069b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28bc98d7fdb41ceb5b3e6b84876cb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c39fcad4dd434abfe2d17038c3f0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef58a1079144b998fb41cf5d51230eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = [] # stores actual image data\n",
    "labels = [] # stores labels (as integer - because this is what our network needs)\n",
    "label_names = [] # maps label ints to their actual categories so we can understand predictions later\n",
    "\n",
    "# loop over all conditions\n",
    "# loop over all files in the condition's directory\n",
    "# read the image and corresponding annotation\n",
    "# crop image to the region of interest\n",
    "# preprocess image\n",
    "# store preprocessed image and label in corresponding lists\n",
    "for condition in CONDITIONS:\n",
    "    for filename in tqdm(os.listdir(\"../../gesture_dataset_sample/\"+condition)):\n",
    "        # extract unique ID from file name\n",
    "        UID = filename.split('.')[0]\n",
    "        img = cv2.imread(f'../../gesture_dataset_sample/{condition}/{filename}')\n",
    "        \n",
    "        # get annotation from the dict we loaded earlier\n",
    "        try:\n",
    "            annotation = annotations[condition][UID]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # iterate over all hands annotated in the image\n",
    "        for i, bbox in enumerate(annotation['bboxes']):\n",
    "            # annotated bounding boxes are in the range from 0 to 1\n",
    "            # therefore we have to scale them to the image size\n",
    "            x1 = int(bbox[0] * img.shape[1])\n",
    "            y1 = int(bbox[1] * img.shape[0])\n",
    "            w = int(bbox[2] * img.shape[1])\n",
    "            h = int(bbox[3] * img.shape[0])\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "            \n",
    "            # crop image to the bounding box and apply pre-processing\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            preprocessed = preprocess_image(crop)\n",
    "            \n",
    "            # get the annotated hand's label\n",
    "            # if we have not seen this label yet, add it to the list of labels\n",
    "            label = annotation['labels'][i]\n",
    "            if label not in label_names:\n",
    "                label_names.append(label)\n",
    "            \n",
    "            label_index = label_names.index(label)\n",
    "            \n",
    "            images.append(preprocessed)\n",
    "            labels.append(label_index)\n",
    "\n",
    "images_val = [] # stores actual image data\n",
    "labels_val = [] # stores labels (as integer - because this is what our network needs)\n",
    "\n",
    "# loop over all conditions\n",
    "# loop over all files in the condition's directory\n",
    "# read the image and corresponding annotation\n",
    "# crop image to the region of interest\n",
    "# preprocess image\n",
    "# store preprocessed image and label in corresponding lists\n",
    "for condition in CONDITIONS:\n",
    "    for filename in tqdm(os.listdir(\"./dataset-liu/\"+condition)):\n",
    "        # extract unique ID from file name\n",
    "        UID = filename.split('.')[0]\n",
    "        img = cv2.imread(f'./dataset-liu/{condition}/{filename}')\n",
    "        \n",
    "        # get annotation from the dict we loaded earlier\n",
    "        try:\n",
    "            annotation = annotations_val[condition][UID]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # iterate over all hands annotated in the image\n",
    "        for i, bbox in enumerate(annotation['bboxes']):\n",
    "            # annotated bounding boxes are in the range from 0 to 1\n",
    "            # therefore we have to scale them to the image size\n",
    "            x1 = int(bbox[0] * img.shape[1])\n",
    "            y1 = int(bbox[1] * img.shape[0])\n",
    "            w = int(bbox[2] * img.shape[1])\n",
    "            h = int(bbox[3] * img.shape[0])\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "            \n",
    "            # crop image to the bounding box and apply pre-processing\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            preprocessed = preprocess_image(crop)\n",
    "            \n",
    "            # get the annotated hand's label\n",
    "            # if we have not seen this label yet, add it to the list of labels\n",
    "            label = annotation['labels'][i]\n",
    "            if label not in label_names:\n",
    "                label_names.append(label)\n",
    "            \n",
    "            label_index = label_names.index(label)\n",
    "            \n",
    "            images_val.append(preprocessed)\n",
    "            labels_val.append(label_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097ae97c-8dd4-48b7-8a94-bdc1fbe80346",
   "metadata": {},
   "source": [
    "## let's have a look at one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9b82f39-8412-4f5a-b074-d091fde3a88f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15edce8b810>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(random.sample(images, 1)[0])\n",
    "plt.imshow(random.sample(images_val, 1)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27443d4b-83d1-43d8-9f23-52859703ebd3",
   "metadata": {},
   "source": [
    "## split data set into train and test\n",
    "\n",
    "x is for the actual data, y is for the label (this is convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14fdf1c6-1722-42d7-a353-69cfbdf6d675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293\n",
      "324\n",
      "1293\n",
      "324\n"
     ]
    }
   ],
   "source": [
    "X_train, _ , y_train, _ = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d160d5f-2bd9-4864-9a9e-d5525b55bde6",
   "metadata": {},
   "source": [
    "## transform data sets into a format compatible with our neural network\n",
    "\n",
    "image data has to be a numpy array with following dimensions: [image_id, y_axis, x_axis, color_channels]\n",
    "\n",
    "furthermore, scale all values to a range of 0 to 1\n",
    "\n",
    "training data has to be converted to a categorial vector (\"one hot\"):\n",
    "\n",
    "[3] --> [0, 0, 0, 1, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "243e3d2c-37ad-4596-a431-cdfd55151145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1293, 64, 64, 3) (25, 64, 64, 3) (1293, 6) (25, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train).astype('float32')\n",
    "X_train = X_train / 255.\n",
    "\n",
    "X_test = np.array(images_val).astype('float32')\n",
    "X_test = X_test / 255.\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(labels_val)\n",
    "\n",
    "train_label = y_train_one_hot\n",
    "test_label = y_test_one_hot\n",
    "\n",
    "X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "\n",
    "print(X_train.shape, X_test.shape, train_label.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "489e2741-bb40-4501-819e-7a6c18ebcf17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# variables for hyperparameters\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "num_classes = len(label_names)\n",
    "activation = 'relu'\n",
    "activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "layer_count = 2\n",
    "num_neurons = 64\n",
    "\n",
    "# define model structure\n",
    "# with keras, we can use a model's add() function to add layers to the network one by one\n",
    "model = Sequential()\n",
    "\n",
    "# data augmentation (this can also be done beforehand - but don't augment the test dataset!)\n",
    "model.add(RandomFlip('horizontal'))\n",
    "model.add(RandomContrast(0.1))\n",
    "#model.add(RandomBrightness(0.1))\n",
    "#model.add(RandomRotation(0.2))\n",
    "\n",
    "# first, we add some convolution layers followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(9, 9), activation=activation_conv, input_shape=(SIZE[0], SIZE[1], COLOR_CHANNELS), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "# dropout layers can drop part of the data during each epoch - this prevents overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# after the convolution layers, we have to flatten the data so it can be fed into fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# add some fully connected layers (\"Dense\")\n",
    "for i in range(layer_count - 1):\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# for classification, the last layer has to use the softmax activation function, which gives us probabilities for each category\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# specify loss function, optimizer and evaluation metrics\n",
    "# for classification, categorial crossentropy is used as a loss function\n",
    "# use the adam optimizer unless you have a good reason not to\n",
    "model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# define callback functions that react to the model's behavior during training\n",
    "# in this example, we reduce the learning rate once we get stuck and early stopping\n",
    "# to cancel the training if there are no improvements for a certain amount of epochs\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0401f45-19a6-4447-805c-50516dcad753",
   "metadata": {},
   "source": [
    "## now, we can train the model using the fit() function\n",
    "## this will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71c4c865-ee8f-4385-8ae7-9d0f70802c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 6) and (None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     X_train,\n\u001b[0;32m      3\u001b[0m     train_label,\n\u001b[0;32m      4\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      5\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      6\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_test, test_label),\n\u001b[0;32m      8\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[reduce_lr, stop_early]\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey4i17jt9.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\liu95\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 6) and (None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    train_label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, test_label),\n",
    "    callbacks=[reduce_lr, stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaadcde-ab15-4e1c-bbab-cf359fba6325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's have a look at our model\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a30ac212-8bd0-4a2d-9f9e-4a137d020645",
   "metadata": {},
   "source": [
    "## Plot accuracy and loss of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275092f5-84e2-4a26-850d-3b267f91fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (Line), Loss (Dashes)')\n",
    "\n",
    "ax.axhline(1, color='gray')\n",
    "\n",
    "plt.plot(accuracy, color='blue')\n",
    "plt.plot(val_accuracy, color='orange')\n",
    "plt.plot(loss, '--', color='blue', alpha=0.5)\n",
    "plt.plot(val_loss, '--', color='orange', alpha=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b63f0a8e-d384-4790-bb47-889224eb2130",
   "metadata": {},
   "source": [
    "## saving the model\n",
    "\n",
    "the function will create a directory for your model and save structure and weights in there\n",
    "\n",
    "sometimes you will see the .h5 format being used - even though this is a bit faster and needs less space, it comes with its limitations and isn't used that much any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f21c0-6fed-430b-9f96-1c75ccf5bee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('gesture_recognition')\n",
    "\n",
    "# and this is how you load the model\n",
    "# model = keras.models.load_model(\"gesture_recognition\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "165b3fd0-e2fd-4a10-95c6-1370908b9f0c",
   "metadata": {},
   "source": [
    "## visualize classification results with a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf19f75-976a-449a-bc7a-76a4d30e9c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let the model make predictions for our training data\n",
    "y_predictions = model.predict(X_test)\n",
    "\n",
    "# we get a 2D numpy array with probabilities for each category\n",
    "print('before', y_predictions)\n",
    "\n",
    "# to build a confusion matrix, we have to convert it to classifications\n",
    "# this can be done by using the argmax() function to set the probability to 1 and the rest to 0\n",
    "y_predictions = np.argmax(y_predictions, axis=1)\n",
    "\n",
    "print('probabilities', y_predictions)\n",
    "\n",
    "# create and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot(ax=plt.gca())\n",
    "\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23c5ebfc-d66d-4aa6-9738-2e2b2f7e8c94",
   "metadata": {},
   "source": [
    "## let's test our model in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d94ec-6d88-4b25-92f0-663cfa423a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "if COLOR_CHANNELS == 1:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "cap.release()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b51b9e16-b4a3-4017-bb1f-ce539e3ea61c",
   "metadata": {},
   "source": [
    "## manual cropping for demonstration purposes - you can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b80a4d-e59c-43ab-9581-705feff33eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(frame[50:350, 60:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e91f6-80b0-47a6-aaf7-fc6ef5b1762e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized = cv2.resize(frame[50:350, 60:210], SIZE)#[20:350, 200:420], SIZE)\n",
    "plt.imshow(resized)\n",
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae7ec6-6c50-4fe0-ab59-908e1123bdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped = resized.reshape(-1, IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)\n",
    "reshaped.shape\n",
    "prediction = model.predict(reshaped)\n",
    "\n",
    "print(label_names[np.argmax(prediction)], np.max(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e143812-9d54-4677-8cc7-45039cc20827",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = model.layers[2].get_weights()\n",
    "print(filters.shape)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(20, 20))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        img = filters[:,:,:,i*8+j]\n",
    "        axes[i][j].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0644e2-0739-42e8-a405-37c08f6d30e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(20, 20))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        #axes[i][j].imshow(filters[:,:,:,i*8+j] * 255, 'gray')\n",
    "        kernel = filters[:,:,0,i*8+j]\n",
    "\n",
    "        #print(kernel.shape)\n",
    "\n",
    "        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        filtered = cv2.filter2D(gray, -1, kernel)\n",
    "\n",
    "        axes[i][j].imshow(filtered, 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a7c9144-78d2-4725-944f-1bfd4fc1a97b",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "let's use a pre-trained model (VGG16) for our prediction\n",
    "\n",
    "note that VGG16 needs three color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43fee9-6d18-4b32-807b-3e41d023a3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 50\n",
    "num_classes = len(label_names)\n",
    "activation = 'relu'\n",
    "activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "layer_count = 2\n",
    "num_neurons = 256\n",
    "\n",
    "# load a VGG16 model trained on the imagenet dataset\n",
    "# include_top=False -> do not include the output layer\n",
    "# input_tensor -> tells the model about the dimensions of our images (VGG16 needs three color channels)\n",
    "# pooling -> which type of pooling to use between convolutions; max or avg seem to be the best\n",
    "VGG = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, COLOR_CHANNELS)), pooling='max')\n",
    "\n",
    "# we want to use the VGG's original weights -> make those layers untrainable\n",
    "for layer in VGG.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# build a new model and add the VGG layers\n",
    "model_vgg = Sequential()\n",
    "model_vgg.add(VGG)\n",
    "\n",
    "# append dense layers at the end\n",
    "for i in range(layer_count - 1):\n",
    "    model_vgg.add(Dense(num_neurons, activation=activation))\n",
    "model_vgg.add(Dropout(0.2))\n",
    "\n",
    "model_vgg.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# classifier\n",
    "model_vgg.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_vgg.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=0.0001)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdd148-83c2-401a-a27f-57cc6b61dee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_vgg = model_vgg.fit(\n",
    "    X_train,\n",
    "    train_label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, test_label),\n",
    "    callbacks=[reduce_lr, stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14479f5f-4af0-4506-aaf7-a08ee2fe8b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dd293-b9d0-48bd-ad2f-6f489316e8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = history_vgg.history['loss']\n",
    "val_loss = history_vgg.history['val_loss']\n",
    "accuracy = history_vgg.history['accuracy']\n",
    "val_accuracy = history_vgg.history['val_accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (Line), Loss (Dashes)')\n",
    "\n",
    "ax.axhline(1, color='gray')\n",
    "\n",
    "plt.plot(accuracy, color='blue')\n",
    "plt.plot(val_accuracy, color='orange')\n",
    "plt.plot(loss, '--', color='blue', alpha=0.5)\n",
    "plt.plot(val_loss, '--', color='orange', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d4897-7d12-4ff1-9004-8b3d04d2df8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let the model make predictions for our training data\n",
    "y_predictions = model_vgg.predict(X_test)\n",
    "\n",
    "# we get a 2D numpy array with probabilities for each category\n",
    "print('before', y_predictions)\n",
    "\n",
    "# to build a confusion matrix, we have to convert it to classifications\n",
    "# this can be done by using the argmax() function to set the probability to 1 and the rest to 0\n",
    "y_predictions = np.argmax(y_predictions, axis=1)\n",
    "\n",
    "print('probabilities', y_predictions)\n",
    "\n",
    "# create and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot(ax=plt.gca())\n",
    "\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2373dc1-5a13-462f-a732-f3fdbec12304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
